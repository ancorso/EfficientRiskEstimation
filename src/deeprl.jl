using Crux
using LinearAlgebra
using Distributions
using Random
using ImportanceWeightedRiskMetrics

# function value_estimate(Ï€::DiscreteNetwork, P, s)
# 	pdfs = Crux.logits(P, s)
# 	sum(value(Ï€, s) .* pdfs, dims=1)
# end

# function PVaR_target(Ï€, ğ’Ÿ, P, var_target)
# 	return ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> var_target) .+ (1.f0 .- ğ’Ÿ[:done]) .* value_estimate(Ï€, P, ğ’Ÿ[:sp])
# end
# 
# function abs_err_pf(Ï€, ğ’Ÿ, ys)
# 	abs.(value(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a])  .-  ys)
# end

# Training
function trainer(ğ’®; elite_frac=0.1, Î±scale = 1.1f0)
	function train_dist(;d, xs, fs, ws, Î±, P, kwargs...)
		d = d isa MISDistribution ? d.distributions[end] : d #NOTE: Need to have the policy as the last distribution
		
		# Compute VaR Estimate
		risk_metric = IWRiskMetrics(fs, ws, Î±scale*Î±)
		var_min = sort(fs, rev=true)[floor(Int, length(fs) * elite_frac)]
		ğ’®.ğ’«[:var_target][1] = min(risk_metric.var, var_min)
		var_info = Dict(:curr_var_est => risk_metric.var, :elite_estimate => var_min)
		
		# Construct the experience buffer
		ğ’Ÿ = Dict(k => hcat([d[k] for d in xs]...) for k in keys(xs[1]))
		ğ’Ÿ[:return] = hcat([fill(Float32(f), 1, length(x[:r])) for (f, x) in zip(fs, xs)]...)
		ğ’Ÿ[:cum_importance_weight] = hcat([fill(Float32(w), 1, length(x[:r])) for (w, x) in zip(ws, xs)]...)
		ğ’Ÿ = ExperienceBuffer(ğ’Ÿ)
		
		# Train the actor
		info = Dict()
		Crux.batch_train!(actor(d.sampler.agent.Ï€), ğ’®.a_opt, ğ’®.ğ’«, ğ’Ÿ, info=info, Ï€_loss=d.sampler.agent.Ï€)
		
		# Train the critic (if applicable)
		if !isnothing(ğ’®.c_opt)
			Crux.batch_train!(critic(d.sampler.agent.Ï€), ğ’®.c_opt, ğ’®.ğ’«, ğ’Ÿ, info=info)
		end
		
		# Log the results
		ğ’®.log.sampler = d.sampler
        Crux.log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, info, var_info, ğ’®=ğ’®)
    	ğ’®.i += ğ’®.Î”N
		
		d
	end
end

# Loss function
function var_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict())
	new_probs = logpdf(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a])
	# nom_probs = logpdf(P.Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a])
	e_loss = -mean(entropy(Ï€, ğ’Ÿ[:s]))
	
	Flux.Zygote.ignore() do
		info[:entropy] = -e_loss
		info[:kl] = mean(ğ’Ÿ[:logprob] .- new_probs)
		info[:mean_weight] = mean(ğ’Ÿ[:cum_importance_weight])
		info[:target] = ğ’«[:var_target][1]
		
	end 
	
	-mean(new_probs .* ((ğ’Ÿ[:return] .> ğ’«[:var_target][1]) .* ğ’Ÿ[:cum_importance_weight] .- Float32(Î±))) #+ 0.01f0*e_loss
	# -mean(new_probs .* ((ğ’Ÿ[:return] .> ğ’«[:var_target][1]) .* ğ’Ÿ[:cum_importance_weight] .- value(Ï€, ğ’Ÿ[:s]))) #+ 0.01f0*e_loss
	# -mean(new_probs .* (ğ’Ÿ[:return] .> ğ’«[:var_target][1]))
end

# function baseline_loss(Ï€, ğ’«, D; kwargs...)
# 	Flux.mse(value(Ï€, D[:s]), ((D[:return] .> ğ’«[:var_target][1]) .* D[:cum_importance_weight]))
# end

function policy_match_logits(P)
    logps = log.(Float32.(P.distribution.p))
    (Ï€, s) -> softmax(log.(softplus.(value(Ï€, s))) .+ logps)
end

function DeepSampler(;Px, mdp, Ï€fn, elite_frac=0.1, Î±scale=1.1f0, actor_batch_size=1024, target_kl=0.1f0)
    # Solver setup for training
    ğ’® = OnPolicySolver(;agent=PolicyParams(Ï€=Ï€fn(), pa=Px),
                    S=state_space(mdp),
                    ğ’« = (;var_target=[0.0]),
                    log = LoggerParams(;dir="log/DeepAMIS"),
                    a_opt = TrainingParams(;loss=var_loss, name = "actor_", batch_size=actor_batch_size, early_stopping = (infos) -> (infos[end][:kl] > target_kl)),
                    # c_opt=TrainingParams(;loss=baseline_loss, name = "critic_", max_batches=50, batch_size=1024),
                    required_columns = unique([:logprob, :importance_weight]))
                    
    sampler = Sampler(mdp, ğ’®.agent, S=ğ’®.S, required_columns=ğ’®.required_columns)
    ğ’®.log.sampler = sampler
    
    return (d0=MDPSampler(sampler), update_distribution=trainer(ğ’®; elite_frac=elite_frac, Î±scale=Î±scale))
end



